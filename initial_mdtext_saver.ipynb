{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999024c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a26eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = Path().absolute()\n",
    "read_path = read_path / 'folder_data.xlsx'\n",
    "try:\n",
    "    data_df = pd.read_excel(read_path)\n",
    "except:\n",
    "    print(\"Could not read {}\".format(read_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621776be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.rename(columns={'FinalFolder':'Project'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e20bc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Project</th>\n",
       "      <th>Series</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Programming</th>\n",
       "      <th>Python</th>\n",
       "      <th>R</th>\n",
       "      <th>RDBMS &amp; SQL</th>\n",
       "      <th>SQL (MySQL)</th>\n",
       "      <th>SQL (PostgreSQL)</th>\n",
       "      <th>...</th>\n",
       "      <th>Unsupervised ML</th>\n",
       "      <th>Semi-supervised ML</th>\n",
       "      <th>cross validation</th>\n",
       "      <th>grid search</th>\n",
       "      <th>NLP</th>\n",
       "      <th>Image Recognition</th>\n",
       "      <th>Text Recognition</th>\n",
       "      <th>Sentiment Analysis</th>\n",
       "      <th>Communication</th>\n",
       "      <th>Technical Writing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\dmark\\OneDrive\\Career\\GitHub\\Capstone...</td>\n",
       "      <td>CapstoneProject_Full_Data_Engineering</td>\n",
       "      <td>IBM Data Engineering &amp; Python</td>\n",
       "      <td>In this project, I applied all of the skills a...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  \\\n",
       "0  C:\\Users\\dmark\\OneDrive\\Career\\GitHub\\Capstone...   \n",
       "\n",
       "                                 Project                         Series  \\\n",
       "0  CapstoneProject_Full_Data_Engineering  IBM Data Engineering & Python   \n",
       "\n",
       "                                             Summary  Programming  Python  \\\n",
       "0  In this project, I applied all of the skills a...         True    True   \n",
       "\n",
       "       R  RDBMS & SQL  SQL (MySQL)  SQL (PostgreSQL)  ...  Unsupervised ML  \\\n",
       "0  False         True         True              True  ...            False   \n",
       "\n",
       "   Semi-supervised ML  cross validation  grid search    NLP  \\\n",
       "0               False             False        False  False   \n",
       "\n",
       "   Image Recognition  Text Recognition  Sentiment Analysis  Communication  \\\n",
       "0              False             False               False           True   \n",
       "\n",
       "   Technical Writing  \n",
       "0               True  \n",
       "\n",
       "[1 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea79f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_listed_skill = 'Programming'\n",
    "last_listed_skill = 'Technical Writing'\n",
    "fls_loc = list(data_df.columns).index(first_listed_skill)\n",
    "lls_loc = list(data_df.columns).index(last_listed_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a6f27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_cols = data_df.columns[fls_loc:lls_loc+1]\n",
    "for index in data_df.index:\n",
    "    data_df.at[index,'Skills'] = \", \".join(list(skills_cols[data_df.iloc[index,fls_loc:lls_loc+1].values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb226813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\CapstoneProject_Full_Data_Engineering',\n",
       "  '## Project(CapstoneProject_Full_Data_Engineering)\\n### Part of the Coursera series: IBM Data Engineering & Python\\n    \\n## Summary\\nIn this project, I applied all of the skills and knowledge gained during the courses leading up to it.   We were tasked with taking in OLTP data via reading a .csv file as well as querying a SQL (MySQL) database.  This data was then exported for additional querying and manipulatoin in a NoSQL database (MongoDB).  We then agglomerated the data in a datawarehoues and performed addional SQL queries and manipulation, this time using PostgreSQL.  On the data, we created some visualizations before setting up a pipeline to handle automation of ETL going forward,  and we ended the project by developing an automated process to create a machine learning model to predict future behavior.\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, RDBMS & SQL, SQL (MySQL), SQL (PostgreSQL), SQL (SQLite), NoSQL (Cassandra), NoSQL (MongoDB), Databases, Statistics, Probability, Linear Algebra, SciPy, Numpy, Pandas, Seaborn, Matplotlib, Plotly, BeautifulSoup, Dataframes, ETL &| ELT & Data Pipelines, DAGs, Apache Airflow, Apache Kafka, Apache Spark, Apache Hadoop, Automation, Linux/Bash/Shell Commands, Webscraping, APIs, Data Modeling, EDA, Data Visualization, Data Summarization, Data Reporting, Regression, Supervised ML, Communication, Technical Writing\\n    '),\n",
       " 1: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_Data-Analysis-Apache-Spark',\n",
       "  '## Project(Project_Data-Analysis-Apache-Spark)\\n### Part of the Coursera series: IBM Data Engineering & Python\\n    \\n## Summary\\nMy Jupyter notebook did not save correctly, unfortunately.  My work has been deleted, but the outline of the project belies the steps I understook.  Primarily, we completed these tasks:\\nTask 1: Generate DataFrame from CSV data.\\nTask 2: Define a schema for the data.\\nTask 3: Display schema of DataFrame.\\nTask 4: Create a temporary view.\\nTask 5: Execute an SQL query.\\nTask 6: Calculate Average Salary by Department.\\nTask 7: Filter and Display IT Department Employees.\\nTask 8: Add 10% Bonus to Salaries.\\nTask 9: Find Maximum Salary by Age.\\nTask 10: Self-Join on Employee Data.\\nTask 11: Calculate Average Employee Age.\\nTask 12: Calculate Total Salary by Department.\\nTask 13: Sort Data by Age and Salary.\\nTask 14: Count Employees in Each Department.\\nTask 15: Filter Employees with the letter o in the Name. \\n\\n## Skills (Developed & Applied)\\nProgramming, Python, RDBMS & SQL, Databases, Statistics, Numpy, Pandas, Dataframes, ETL &| ELT & Data Pipelines, DAGs, Apache Spark, Automation, Data Modeling, EDA, Data Visualization, Data Summarization\\n    '),\n",
       " 2: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_Apache-Airflow-Kafka-on-Toll-Booth-Data',\n",
       "  '## Project(Project_Apache-Airflow-Kafka-on-Toll-Booth-Data)\\n### Part of the Coursera series: IBM Data Engineering & Python\\n    \\n## Summary\\nIn this project, we utilized both a SQL database and Apache Kafka and Apache Spark to read in live data from a toll booth.  We wrangled the data and converted it to the appropriate format, and then inserted it in real time into a SQL database.\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, RDBMS & SQL, SQL (MySQL), Databases, Numpy, Pandas, ETL &| ELT & Data Pipelines, DAGs, Apache Airflow, Apache Kafka, Automation, Data Modeling, EDA, Data Summarization\\n    '),\n",
       " 3: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_Banks-Web-Scraping-SQL',\n",
       "  '## Project(Project_Banks-Web-Scraping-SQL)\\n### Part of the Coursera series: IBM Data Engineering & Python\\n    \\n## Summary\\nWe scraped website data about world banks and then manipulated the data in order to load it into a SQL database, all via API calls in Jupyter Notebooks.  We had to wrangle and transform the data; we took every step in the extract, transform, load process.\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, RDBMS & SQL, SQL (SQLite), Databases, Numpy, Pandas, BeautifulSoup, ETL &| ELT & Data Pipelines, Webscraping, APIs\\n    '),\n",
       " 4: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_Linux-File-Backup',\n",
       "  '## Project(Project_Linux-File-Backup)\\n### Part of the Coursera series: IBM Data Engineering & Python\\n    \\n## Summary\\nRather than work in Python, we took directly to the Linux Shell commands in order to draw data from files and folders.  We manipulated this information slighlty before then archiving the files in order to create a backup.  We automated this process as well by setting up the crontab functionality.\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, ETL &| ELT & Data Pipelines, Linux/Bash/Shell Commands\\n    '),\n",
       " 5: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_Airfoil-Noise-Prediction',\n",
       "  '## Project(Project_Airfoil-Noise-Prediction)\\n### Part of the Coursera series: IBM Data Engineering & Python\\n    \\n## Summary\\nI took on the role of data engineer at an aeronautics consulting company. This fictional  company prides itself in being able to efficiently design airfoils for use in planes and sports cars. Data scientists in the office need to work with different algorithms and data in different formats. While they are good at Machine Learning, they counted on me to be able to do ETL jobs and build ML pipelines. In this project I used a modified version of the NASA Airfoil Self Noise dataset. I cleaned this dataset, by dropping the duplicate rows, and removing the rows with null values. I then created an ML pipe line to create a model that predicted the SoundLevel based on all the other columns. I evaluated the model and then persisted it for future use.  Here were the steps:\\n- Part 1 Perform ETL activity\\n - Load a csv dataset\\n - Remove duplicates if any\\n - Drop rows with null values if any\\n - Make transformations\\n - Store the cleaned data in parquet format\\n- Part 2 Create a Machine Learning Pipeline\\n - Create a machine learning pipeline for prediction\\n- Part 3 Evaluate the Model\\n - Evaluate the model using relevant metrics\\n- Part 4 Persist the Model\\n - Save the model for future production use\\n - Load and verify the stored model\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, Statistics, Linear Algebra, Numpy, Pandas, ETL &| ELT & Data Pipelines, Apache Spark, Automation, APIs, Data Modeling, Data Summarization, Regression, Supervised ML\\n    '),\n",
       " 6: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\CapstoneProject_SpaceX_Predictions',\n",
       "  '## Project(CapstoneProject_SpaceX_Predictions)\\n### Part of the Coursera series: IBM Data Science\\n    \\n## Summary\\nIn this capstone, I predicted if the Falcon 9 first stage will land successfully. SpaceX advertises Falcon 9 rocket launches on its website with a cost of 62 million dollars; other providers cost upward of 165 million dollars each, much of the savings is because SpaceX can reuse the first stage. Therefore determining if the first stage will land and the cost of launch can help to determine if an alternate company may want to bid against SpaceX for a rocket launch. In this lab, I completed several stesp:\\n- I collected data from an API and wrangled it into the right format\\n- I gathered additional supplemental data via webscraping\\n- I utilized a SQL database and Python/Juptyer Notebooks to perform EDA.\\n- I created a number of useful visualizations\\n- I employed Folium as a way to visualize launch sites and landmarks\\n- I created a dashboard with Dash and Plotly for stakeholder use\\n- I set up ML models to predict launch success and performed some feature engineering as well\\n- I created a written report summarizing our findings and recommendations.\\n-\\n\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, RDBMS & SQL, SQL (MySQL), Databases, Statistics, Probability, Linear Algebra, Numpy, Pandas, Seaborn, Matplotlib, Plotly, Dash, Folium, BeautifulSoup, Scikit-learn, Dataframes, ETL &| ELT & Data Pipelines, Automation, Webscraping, APIs, Data Modeling, EDA, Data Visualization, Data Summarization, Data Reporting, Dashboards, Classification, Supervised ML, Communication, Technical Writing\\n    '),\n",
       " 7: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_ML-Model-Eval-Refine',\n",
       "  '## Project(Project_ML-Model-Eval-Refine)\\n### Part of the Coursera series: IBM Data Science\\n    \\n## Summary\\nIn this project, I took in data related to home sales in order to develop a model to predict future home sales.  We had to wrnagle the data and transform it, perform EDA and look at various correlations between features in order to set up a machine learning (polynomial linear regression) model on which to train and then create predictions.  We performed feature engneering and scaling in the process.\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, Databases, Statistics, Probability, SciPy, Numpy, Pandas, Seaborn, Matplotlib, Scikit-learn, Data Modeling, EDA, Data Visualization, Data Summarization, Data Reporting, Regression, Supervised ML, Communication\\n    '),\n",
       " 8: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_ML-Classification',\n",
       "  '## Project(Project_ML-Classification)\\n### Part of the Coursera series: IBM Data Science\\n    \\n## Summary\\nI used classification algorithms to create a model based on a set of training data and evaluated our testing data to determine the best model to use for prediction.  I used several algorithms (Linear Regression, KNN, Decision Trees, Logistic Regression, and SVM).  I evaluated these models using Accuracy Score, Jaccard Index, F1-Score, LogLoss, Mean Absolute Error, Mean Squared Error, R2-Score).  \\n\\n## Skills (Developed & Applied)\\nProgramming, Python, Numpy, Pandas, Scikit-learn, Dataframes, Data Modeling, Classification, Supervised ML, Communication\\n    '),\n",
       " 9: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_Boston-Data-Project',\n",
       "  '## Project(Project_Boston-Data-Project)\\n### Part of the Coursera series: IBM Data Science\\n    \\n## Summary\\nWe inputted a large dataset and ran a number of statistical analyses to better understand what messages the data may be telling us.  We performed hypothesis testing to see if certain observations were statistically significant. \\n\\n## Skills (Developed & Applied)\\nProgramming, Python, Statistics, Probability, Hypothesis Testing, SciPy, Numpy, Pandas, Seaborn, Matplotlib, Dataframes, Data Modeling, EDA, Data Visualization, Data Summarization, Data Reporting, Communication\\n    '),\n",
       " 10: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_Automobile-Sales-Visualization',\n",
       "  '## Project(Project_Automobile-Sales-Visualization)\\n### Part of the Coursera series: IBM Data Science\\n    \\n## Summary\\nIn this project, we were tasked with creating plots which answer questions for analysing \"historical_automobile_sales\" data to understand the historical trends in automobile sales during recession periods.  We employed a number of data wrangling techniques before performing EDA adn then setting up a dashboard to visualize the results in realtime.\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, SciPy, Numpy, Pandas, Seaborn, Matplotlib, Plotly, Dash, Folium, Dataframes, ETL &| ELT & Data Pipelines, Data Modeling, EDA, Data Visualization, Data Summarization, Data Reporting, Dashboards, Communication\\n    '),\n",
       " 11: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_Image-Text-Recognition',\n",
       "  '## Project(Project_Image-Text-Recognition)\\n### Part of the Coursera series: University of Michigan Python 3 Programming\\n    \\n## Summary\\nIn this project, we took in a number of images and, using existing computer vision packages in Python, we were able to pick out important text and recognize images, pairing them together.\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, Numpy, Classification, Supervised ML, Image Recognition, Text Recognition\\n    '),\n",
       " 12: ('C:\\\\Users\\\\dmark\\\\OneDrive\\\\Career\\\\GitHub\\\\Project_Sentiment-Analysis',\n",
       "  '## Project(Project_Sentiment-Analysis)\\n### Part of the Coursera series: University of Michigan Python 3 Programming\\n    \\n## Summary\\nIn this project, we inputted a number of tweets and analyzed their content in order to perform rudimetary sentiment analysis.\\n\\n## Skills (Developed & Applied)\\nProgramming, Python, Statistics, Webscraping, Classification, Sentiment Analysis\\n    ')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_dic = {}\n",
    "for index in list(data_df.index):\n",
    "    path = data_df.loc[index,'Path']\n",
    "    series = data_df.loc[index,'Series']\n",
    "    project = data_df.loc[index,'Project']\n",
    "    skills = data_df.loc[index,'Skills']\n",
    "    summary = data_df.loc[index,'Summary']\n",
    "    text = f\"\"\"## Project({project})\n",
    "### Part of the Coursera series: {series}\n",
    "    \n",
    "## Summary\n",
    "{summary}\n",
    "\n",
    "## Skills (Developed & Applied)\n",
    "{skills}\n",
    "    \"\"\"\n",
    "    rm_dic[index] = path, text\n",
    "rm_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f5ef0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (folder_path, text) in rm_dic.items():\n",
    "#     readme = 'README'+str(index)+'.md'\n",
    "#     temp_path = r'C:\\Users\\dmark\\OneDrive\\Career\\Projects\\README Generator\\TEMP'\n",
    "#     write_path = Path(temp_path) / readme\n",
    "    write_path = Path(folder_path) / \"README.md\"\n",
    "    try:\n",
    "        write_path.unlink()\n",
    "    except:\n",
    "        pass\n",
    "    write_path.touch()\n",
    "    write_path.write_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c563d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
